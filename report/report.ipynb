{
  "cells": [
    {
      "cell_type": "raw",
      "id": "605139f9",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Report:Agrisage\"\n",
        "author: \"Subhangi Pandey\"\n",
        "output: html\n",
        "format: \n",
        "  html:\n",
        "    code-fold: true\n",
        "    toc: true\n",
        "    toc-depth: 2 \n",
        "    toc-title: \"Contents\"\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac62e43d",
      "metadata": {},
      "source": [
        "## Abstract\n",
        "\n",
        "This project focuses on the development and implementation of a predictive model for crop selection based on a comprehensive agricultural dataset. The dataset comprises multiple features such as nutrients in soil, weather conditions, and other agronomic factors, with the target variable being the crop type.\n",
        "\n",
        "The primary objective was to analyze the dataset to understand the key factors influencing crop selection, followed by the construction of a predictive model to recommend the most suitable crops for given conditions. Various data preprocessing techniques and exploratory data analysis (EDA) were employed to enhance the quality of the data.\n",
        "\n",
        "Multiple machine learning algorithms were evaluated, including decision trees, random forests, and logistic regression, to determine the most accurate model. The final model was selected based on its accuracy.\n",
        "\n",
        "Additionally, a Streamlit application was developed to provide an interactive user interface for stakeholders, enabling them to input specific agronomic conditions and receive crop recommendations with associated probabilities. This tool aims to support farmers and agricultural planners in making informed decisions to optimize crop yields and sustainability. Additionally, it ensures the validity of the model presented.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "We have a dataset that showcases what crop is best suited for a combination of different features(quantitatively) like Nitrogen, Phosphorus, Potassium and so on.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd3b417e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.gridspec import GridSpec, GridSpecFromSubplotSpec\n",
        "import seaborn as sb\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer \n",
        "from sklearn.discriminant_analysis import StandardScaler \n",
        "from sklearn.preprocessing import QuantileTransformer \n",
        "from scipy.stats import boxcox, yeojohnson\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24e174e2",
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path=r\"C:\\Users\\subha\\Desktop\\AgriSage\\Data\\Crop_Recommendation.csv\"\n",
        "raw_data= pd.read_csv (file_path)\n",
        "raw_data.info()\n",
        "target = 'Crop'\n",
        "features= raw_data.columns[:-1]\n",
        "a1 = raw_data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7cb0734",
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "# Suppressing specific warning\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Suppressing all warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e37da91",
      "metadata": {},
      "source": [
        "## Exploratory Data Analysis (EDA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ac7e2be",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_counts(data, features, hue=None):\n",
        "    n_cols = 2 \n",
        "    n_rows = int(np.ceil(len(features)/n_cols)) \n",
        "\n",
        "    # Create figure\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4*n_rows))\n",
        "    # Flatten the axes array for easier indexing\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, feature in enumerate(features):\n",
        "        #bar_container = axes[i].bar(df[feature].value_counts().index, df[feature].value_counts().values)\n",
        "        sb.countplot(\n",
        "        data = data,\n",
        "        x = feature,\n",
        "        palette = 'Paired',\n",
        "        hue = hue,\n",
        "        ax = axes[i]\n",
        "        )\n",
        "        for p in axes[i].patches:\n",
        "            label = p.get_height()\n",
        "            x = p.get_x() + p.get_width() / 2 # Width of the plot\n",
        "            y = p.get_height() # Height of the plot\n",
        "            \n",
        "            rotation = 0\n",
        "            if hue is not None:\n",
        "                rotation = 30  # Rotate annotations by 30 degrees if hue is not None\n",
        "\n",
        "    axes[i].annotate(\n",
        "        '{:.0f}'.format(label),\n",
        "        (x, y),\n",
        "        ha = 'center',\n",
        "        va = 'center',\n",
        "        size = 12,\n",
        "        xytext = (0, 5),\n",
        "        textcoords = 'offset points',\n",
        "        rotation = rotation\n",
        "    )\n",
        "    axes[i].set(ylabel='Count', title=feature, xlabel='')\n",
        "    axes[i].tick_params(axis='x', rotation=30)\n",
        "\n",
        "    # If the number of features is odd, remove the empty subplot\n",
        "    if len(features) % 2 != 0:\n",
        "        fig.delaxes(axes[-1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plot_counts(raw_data, [target])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0019220f",
      "metadata": {},
      "source": [
        "The above figure helps us in a clear visualization of our data. \n",
        "\n",
        "1. Quantification: We have 22 possible targets that is crops for our datset and since each crop has a 100 samples, we have 2200 samples.\n",
        "\n",
        "2. Class Balance: The fact that each crop has an equal number of samples (100) suggests that the dataset is balanced. This is beneficial for training machine learning models as it can prevent bias towards any particular class.\n",
        "\n",
        "3. Granularity: In a classification problem like the one presented here, having 100 samples per target allows for a detailed representation of each class.\n",
        "\n",
        "However, the number of samples is a subject to change after further EDA.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d517730",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 10))\n",
        "sb.heatmap(raw_data.corr(numeric_only=True), annot=True, fmt='.2f')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f275007",
      "metadata": {},
      "source": [
        "The above figures tells us that Phosphorus and Potassium have a high positive correation. This can be due to several underlying reasons related to soil chemistry, plant physiology, and farming practices. Here are some common reasons why these two nutrients might show correlation:\n",
        "\n",
        "1. Soil Composition: Phosphorus (P) and potassium (K) availability in soil can be influenced by similar factors such as soil type, pH levels, and organic matter content.\n",
        "\n",
        "2. Fertilizer Application: Farmers often apply fertilizers that contain both phosphorus and potassium together. This simultaneous application can lead to their concentrations being correlated in the soil.\n",
        "\n",
        "3. Sampling and Analysis: Sometimes, the correlation observed could be due to the way samples are collected or analyzed. If samples are taken from similar locations or depths within a field, they may show similar nutrient profiles.\n",
        "\n",
        "4. Environmental Factors: Environmental conditions such as rainfall, temperature, and humidity can affect the mobility and availability of both phosphorus and potassium in the soil. Similar environmental impacts can result in correlated values. In our datset since these factors would be same for a particular area, the correlation is understandable.\n",
        "\n",
        "Highly correlated variables can adversely affect the performance of predictive models, particularly in regression and classification tasks:\n",
        " \n",
        "1. It can lead to multicollinearity, where the coefficients become unstable and difficult to interpret in regression models.\n",
        "\n",
        "2. It can also lead to OVERFITTING occurs when the model captures noise or random fluctuations rather than underlying patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb96c7df",
      "metadata": {},
      "outputs": [],
      "source": [
        "def dist_plot(data, feature_list):\n",
        "    n_cols= 2 \n",
        "    n_rows = int(np.ceil(len(feature_list)/n_cols)) \n",
        "    # Creating figure\n",
        "    fig = plt.figure(figsize=(16, 4*n_rows))\n",
        "    outer = GridSpec(n_rows, n_cols, wspace=0.2, hspace=0.3)\n",
        "\n",
        "    for i in range(len(feature_list)):\n",
        "        inner = GridSpecFromSubplotSpec(2, 1, subplot_spec=outer[i], \n",
        "                                                 wspace=0.1, hspace=0.1, height_ratios=(0.15, 0.85))\n",
        "        ax_box= plt.Subplot(fig, inner[0])\n",
        "        sb.boxplot(data=data, x=feature_list[i], color='lightblue', ax=ax_box)\n",
        "        ax_box.set_xlabel('')\n",
        "        fig.add_subplot(ax_box)\n",
        "\n",
        "        mean_value = data[feature_list[i]].mean()\n",
        "        median_value = data[feature_list[i]].median()\n",
        "        ax_hist = plt.Subplot(fig, inner[1])\n",
        "        sb.histplot(data=data, x=feature_list[i], kde=True, ax=ax_hist)\n",
        "        ax_hist.axvline(mean_value, color='green', linestyle='dotted', linewidth=2, label='Mean')\n",
        "        ax_hist.axvline(median_value, color='purple', linestyle='dotted', linewidth=2, label='Median')\n",
        "        ax_hist.legend(loc='lower right', fontsize=10)\n",
        "\n",
        "        # Calculate skewness and kurtosis\n",
        "        skewness = data[feature_list[i]].skew()\n",
        "        kurt = data[feature_list[i]].kurt()\n",
        "        if skewness < 0:\n",
        "            x=0.25\n",
        "        else:\n",
        "            x=0.95\n",
        "        # Add skewness and kurtosis as text on the histogram plot\n",
        "        ax_hist.text(x, 0.85, f\"Skewness: {skewness:.2f}\\nKurtosis: {kurt:.2f}\", \n",
        "                         transform=ax_hist.transAxes, verticalalignment='top', horizontalalignment='right',\n",
        "                         bbox=dict(facecolor='white', edgecolor='gray', boxstyle='round,pad=0.5'),\n",
        "                    fontsize=10)\n",
        "        fig.add_subplot(ax_hist)\n",
        "    plt.tight_layout()\n",
        "    plt.show(block=False)\n",
        "dist_plot(raw_data, features)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cff7ac3f",
      "metadata": {},
      "source": [
        "From the above graph, we are able to easily identify the outliers that need to be dealt with as they can negatively influence the performance of machine learning models. These are a few outliers:\n",
        "\n",
        "1. 100 > Potassium> 200\n",
        "\n",
        "2. 15> Temperature >40\n",
        "\n",
        "3. 4> pH_Value >8\n",
        "\n",
        "4. count > 250\n",
        "\n",
        "By addressing outliers, we can improve the robustness and reliability of your models. It can also help in better visualization.\n",
        "\n",
        "## Handling Outliers\n",
        "\n",
        "The Interquartile Range (IQR) method is a common statistical technique used to identify and handle outliers in a dataset. The IQR is the range between the first quartile (Q1) and the third quartile (Q3) of the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53c37a73",
      "metadata": {},
      "outputs": [],
      "source": [
        "Q1= a1[features].quantile(0.25)\n",
        "Q3 = a1[features].quantile(0.75)\n",
        "IQR = Q3 - Q1 \n",
        "\n",
        "\n",
        "print('25th percentile of the given data is \\n', Q1)\n",
        "print('75th percentile of the given data is \\n', Q3)\n",
        "print('Interquartile range is\\n', IQR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bac28a3b",
      "metadata": {},
      "outputs": [],
      "source": [
        "lower_bound= Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "print('lower bound of the given data is \\n', lower_bound)\n",
        "print('upper bound of the given data is \\n', upper_bound)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bbfaf41",
      "metadata": {},
      "outputs": [],
      "source": [
        "outliers = (a1[features] < lower_bound) | (a1[features] > upper_bound)\n",
        "\n",
        "a1_no_outliers = a1[~outliers.any(axis=1)]\n",
        "\n",
        "a1_no_outliers.reset_index(inplace=True, drop=True)\n",
        "\n",
        "# Displaying the cleaned data\n",
        "a1_no_outliers.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcaf3ef8",
      "metadata": {},
      "source": [
        "These are the number of crop samples left after removing the outliers along with graph:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89fae874",
      "metadata": {},
      "outputs": [],
      "source": [
        "a1_no_outliers[target].value_counts()\n",
        "plot_counts(a1_no_outliers, [target])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5958c4a4",
      "metadata": {},
      "source": [
        "It is clear to us that we cannot remove outliers as it leads to serious reduction of samples for some crops like rice and papaya. Reducing the number of samples can decrease the statistical power of our analysis. With fewer samples, the ability to detect true patterns or relationships in the data diminishes.\n",
        "\n",
        "Retaining outliers might provide a more comprehensive understanding of the factors influencing crop yield, leading to more robust and reliable predictions. \n",
        "\n",
        "We transform the data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9141c69f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def transformation_method(data, feature):\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(12, 9))  # Adjusted for an additional plot\n",
        "\n",
        "    # Flatten the axes array for easier indexing\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Hide unused subplot axes\n",
        "    for ax in axes[6:]:\n",
        "        ax.axis('off')\n",
        "        \n",
        "    # Calculate skewness\n",
        "    # Original Distribution of the feature\n",
        "    sb.histplot(data[feature], kde=True, bins=30, palette='viridis', ax=axes[0])\n",
        "    axes[0].set_title(f'Original {feature} Distribution \\n (Skew: {data[feature].skew():.5f})', fontsize=10)\n",
        "    axes[0].set_xlabel(feature, fontsize=10)\n",
        "    axes[0].set_ylabel('Frequency', fontsize=10)\n",
        "\n",
        "    # Log Transformed feature\n",
        "    data['Log_'+feature] = np.log(data[feature])\n",
        "    sb.histplot(data['Log_'+feature], kde=True, bins=30, color='orange', ax=axes[1])\n",
        "    axes[1].set_title(f\"Log Transformed {feature} \\n (Skew: {data['Log_'+feature].skew():.5f})\", fontsize=10)\n",
        "    axes[1].set_xlabel(f'Log of {feature}', fontsize=10)\n",
        "    axes[1].set_ylabel('Frequency', fontsize=10)\n",
        "\n",
        "    # Square Root Transformed feature\n",
        "    data['Sqrt_'+feature] = np.sqrt(data[feature])\n",
        "    sb.histplot(data['Sqrt_'+feature], kde=True, bins=30, color='blue', ax=axes[2])\n",
        "    axes[2].set_title(f\"Square Root Transformed {feature} \\n (Skew: {data['Sqrt_'+feature].skew():.5f})\", fontsize=10)\n",
        "    axes[2].set_xlabel(f'Square Root of {feature}', fontsize=10)\n",
        "    axes[2].set_ylabel('Frequency', fontsize=10)\n",
        "    \n",
        "    # Box-Cox Transformed feature\n",
        "    # Applying Box-Cox Transformation after checking all values are positive\n",
        "    if (data[feature] <= 0).any():\n",
        "        # Shift the values to make them positive\n",
        "        shift_value = abs(data[feature].min()) + 1\n",
        "        data[feature] += shift_value\n",
        "        print(f\"Values of {feature} were shifted to make them positive.\")\n",
        "    data['BoxCox_'+feature], _ = boxcox(data[feature])\n",
        "    sb.histplot(data['BoxCox_'+feature], kde=True, bins=30, color='red', ax=axes[3])\n",
        "    axes[3].set_title(f\"Box-Cox Transformed {feature} \\n Skew: {data['BoxCox_'+feature].skew():.5f})\", fontsize=10)\n",
        "    axes[3].set_xlabel(f'Box-Cox of {feature}', fontsize=10)\n",
        "    axes[3].set_ylabel('Frequency', fontsize=10)\n",
        "\n",
        "    # Yeo-Johnson Transformed feature\n",
        "    # Applying Yeo-Johnson Transformation\n",
        "    data['YeoJohnson_'+feature], _ = yeojohnson(data[feature])\n",
        "    sb.histplot(data['YeoJohnson_'+feature], kde=True, bins=30, color='purple', ax=axes[4])\n",
        "    axes[4].set_title(f\"Yeo-Johnson Transformed {feature} \\n (Skew: {data['YeoJohnson_'+feature].skew():.5f})\", fontsize=10)\n",
        "    axes[4].set_xlabel(f'Yeo-Johnson of {feature}', fontsize=10)\n",
        "    axes[4].set_ylabel('Frequency', fontsize=10)\n",
        "\n",
        "    # Quantile Transformed feature (Normal Distribution)\n",
        "    # Applying Quantile Transformation to follow a normal distribution\n",
        "    quantile_transformer = QuantileTransformer(output_distribution='normal', random_state=0)\n",
        "    data['Quantile_'+feature] = quantile_transformer.fit_transform(data[feature].values.reshape(-1, 1)).flatten()\n",
        "    sb.histplot(data['Quantile_'+feature], kde=True, bins=30, color='green', ax=axes[5])\n",
        "    axes[5].set_title(f\"Quantile Transformed {feature} (Normal Distn, \\n Skew: {data['Quantile_'+feature].skew():.5f})\", fontsize=10)\n",
        "    axes[5].set_xlabel(f'Quantile Transformed {feature}', fontsize=10)\n",
        "    axes[5].set_ylabel('Frequency', fontsize=10)\n",
        "\n",
        "\n",
        "    plt.tight_layout(pad=3.0)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbf593c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "transformation_method(a1, features[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "922fc347",
      "metadata": {},
      "outputs": [],
      "source": [
        "transformation_method(a1, features[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "221e9409",
      "metadata": {},
      "outputs": [],
      "source": [
        "transformation_method(a1, features[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8bd5129",
      "metadata": {},
      "outputs": [],
      "source": [
        "transformation_method(a1, features[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f649081",
      "metadata": {},
      "outputs": [],
      "source": [
        "transformation_method(a1, features[5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed3825b0",
      "metadata": {},
      "source": [
        "It is evident that the quantile transformation graph looks the best so we'll use quantile transformation on the needed data.\n",
        "\n",
        "However in some of the graphs above, we see that some transformations result in double peaks. This might indicate that crops might be devided in two groups for each feature. To understand this assumption of ours, we can graphs that show crops vs features, i.e. how different crops behave to different values of features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b810c90",
      "metadata": {},
      "outputs": [],
      "source": [
        "n_cols = 2\n",
        "n_rows = 4\n",
        "fig, ax = plt.subplots(n_rows, n_cols, figsize=(16, 4*n_rows))\n",
        "ax = ax.flatten()\n",
        "\n",
        "for i, feature in enumerate(features):\n",
        "    sb.boxplot(data=a1, x=target, y=feature,ax=ax[i] )\n",
        "    ax[i].tick_params(axis='x', rotation=30)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show(block=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "159b2a28",
      "metadata": {},
      "source": [
        "From the above box plots it is now clear to us that crops are be divided into to groups for certain features. \n",
        "\n",
        "For example, the Box-Cox transformation and Yeo-Johnson transformation for Nitrogen shows two peaks. This is further confirmed from the box plot above where almost half of the crops reuire low nitrogen and the other half requires high nitrogen. \n",
        "\n",
        "This is also the case for certain other features like Potassium and Phosphorus. \n",
        "\n",
        "## Label Encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7ad3c92",
      "metadata": {},
      "outputs": [],
      "source": [
        "def transform_data_le(df, target, num_features):\n",
        "    # Ensure num_features is a list\n",
        "    if isinstance(num_features, pd.Index):\n",
        "        num_features = num_features.tolist()\n",
        "\n",
        "    # Encoding target\n",
        "    lbl_encoder = LabelEncoder()\n",
        "    df[target+'_Encoded'] = lbl_encoder.fit_transform(df[target])\n",
        "    \n",
        "    # Assigning features and labels\n",
        "    x = df.drop([target, target+'_Encoded'], axis=1)\n",
        "    y = df[target+'_Encoded']\n",
        "    \n",
        "    # Splitting the dataset into train and test sets\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=101)\n",
        "    \n",
        "    # Accessing the encoded classes\n",
        "    encoded_classes = lbl_encoder.classes_\n",
        "    # Printing the mapping (index corresponds to encoded value, value is the original label)\n",
        "    for i, label in enumerate(encoded_classes):\n",
        "        print(f\"Encoded Value: {i}, Original Label: {label}\")    \n",
        "    \n",
        "    # Standardization and Encoding\n",
        "    # Define transformers for different column types\n",
        "    std_scaler = StandardScaler()\n",
        "    quantile_transformer = QuantileTransformer(output_distribution='normal', random_state=0)\n",
        "\n",
        "    # Combine transformers for specific columns\n",
        "    preprocessor = ColumnTransformer([\n",
        "        (\"num\", std_scaler, num_features),\n",
        "        (\"num_trns\", quantile_transformer, num_features)\n",
        "    ])\n",
        "     # Fit transformers on training data only\n",
        "    preprocessor.fit(x_train)\n",
        "\n",
        "    # Transform train and test data using fitted transformers\n",
        "    x_train_transformed = preprocessor.transform(x_train)\n",
        "    x_test_transformed = preprocessor.transform(x_test)\n",
        "\n",
        "    # Save the test set to CSV files\n",
        "    x_test.to_csv('x_test.csv', index=False)\n",
        "    y_test.to_csv('y_test.csv', index=False)\n",
        "    \n",
        "    return x_train_transformed, x_test_transformed, y_train, y_test, preprocessor, lbl_encoder\n",
        "x_train, x_test, y_train, y_test, preprocessor, lbl_encoder = transform_data_le(raw_data, target, features)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "561f73fd",
      "metadata": {},
      "source": [
        "## Model comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e6913d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
        "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.linear_model import SGDClassifier,  LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "def model_comparison(x, y, models):\n",
        "    names = []\n",
        "    scoring = ['accuracy']\n",
        "    \n",
        "    # Create a dataframe to store the different metric values for each algorithm\n",
        "    df_results = pd.DataFrame(columns=['Algorithm', 'Acc Mean', 'Acc STD'])\n",
        "    results_acc = [] # List of accuracy scores for each fold of each algorithm\n",
        "    \n",
        "    for name, model in models:\n",
        "        names.append(name)\n",
        "        kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=101)\n",
        "        result = cross_validate(model, x, y, cv=kfold, scoring=scoring)\n",
        "        # Mean and standard deviation of Accuracy scores for the algorithm\n",
        "        acc_mean = result['test_accuracy'].mean()\n",
        "        acc_std = result['test_accuracy'].std()\n",
        "        \n",
        "        # Create the row of the results\n",
        "        df_result_row = {'Algorithm': name, 'Acc Mean': acc_mean, 'Acc STD': acc_std}\n",
        "        # Add the row to the results data frame\n",
        "        df_results = pd.concat([df_results, pd.DataFrame([df_result_row])], ignore_index=True)\n",
        "        \n",
        "        results_acc.append(result['test_accuracy'])\n",
        "\n",
        "    df_results = df_results.set_index('Algorithm')\n",
        "    pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "    # Display the mean and standard deviation of all metrics for all algorithms\n",
        "    print(df_results) \n",
        "    # Convert the results_acc dictionary into a DataFrame for plotting\n",
        "    results_acc_df = pd.DataFrame(results_acc).T\n",
        "    print(f\"results_acc_df: {results_acc_df}\")\n",
        "    # Create a box plot for the accuracy results\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    ax = sb.boxplot(data=results_acc_df)\n",
        "    ax.set_title('Model Comparison - Accuracy Scores')\n",
        "    ax.set_xlabel('Algorithm')\n",
        "    ax.set_ylabel('Accuracy')\n",
        "    ax.set_xticklabels(names) \n",
        "    plt.show()\n",
        "\n",
        "models = []\n",
        "models.append(('RFC', RandomForestClassifier()))\n",
        "models.append(('ABC', AdaBoostClassifier()))\n",
        "models.append(('GBC', GradientBoostingClassifier()))\n",
        "models.append(('LR', LogisticRegression())) \n",
        "models.append(('SVC', SVC()))\n",
        "models.append(('DTC', DecisionTreeClassifier()))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('GNB', GaussianNB()))\n",
        "models.append(('XGB', SGDClassifier()))\n",
        "\n",
        "model_comparison(x_train, y_train, models)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5e97e51",
      "metadata": {},
      "source": [
        "We can clearly discard Ada Boost Classifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9a8519c",
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_models = []\n",
        "selected_models.append(('RFC', RandomForestClassifier()))\n",
        "selected_models.append(('GBC', GradientBoostingClassifier()))\n",
        "selected_models.append(('LR', LogisticRegression())) \n",
        "selected_models.append(('SVC', SVC()))\n",
        "selected_models.append(('DTC', DecisionTreeClassifier()))\n",
        "selected_models.append(('KNN', KNeighborsClassifier()))\n",
        "selected_models.append(('GNB', GaussianNB()))\n",
        "selected_models.append(('XGB', SGDClassifier()))\n",
        "\n",
        "model_comparison(x_train, y_train, selected_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17a65ec1",
      "metadata": {},
      "source": [
        "After analyzing the above output, we can finalize Random Forets classifier model because of high accuracy and large size of our dataset.\n",
        "\n",
        "## Finalizing model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb25e1f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = RandomForestClassifier()\n",
        "model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec2a53eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "y_hat = model.predict(x_test)\n",
        "print(classification_report(y_test, y_hat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08d86544",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_test, predictions):\n",
        "    # Plot the confusion matrix\n",
        "    cf_matrix = confusion_matrix(y_test, predictions)\n",
        "    fig = plt.subplots(figsize=(10, 8))\n",
        "    sb.set(font_scale=1.4)\n",
        "    sb.heatmap(cf_matrix, annot=True, fmt='d')\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.show()\n",
        "    \n",
        "    # Reset font scale to default\n",
        "    sb.set(font_scale=1)\n",
        "plot_confusion_matrix(y_test, y_hat)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52ca0078",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "The primary objective of this analysis was to develop a robust predictive model for crop prediction. The Random Forest model achieved an accuracy of 99.4%. This results indicate that the model is effective in predicting crop yields. The model's performance was validated using cross-validation techniques, yielding consistent results across different folds.\n",
        "\n",
        "Compared to other models such as Logistic Regression and Decision Trees, the Random Forest model demonstrated superior performance.\n",
        "\n",
        "The implementation of this Random Forest model can significantly improve agricultural planning and decision-making, leading to better resource allocation. For instance, the model can help in accurately predicting crop yields, thereby enhancing operational efficiency. In conclusion, the Random Forest model developed in this analysis provides a powerful tool for predicting crop yields, with substantial potential for practical application in agriculture. \n",
        "\n",
        "An app has also been developed which takes user input for features and then predicts the top five crops. The app also allows us to validate the model chosen by generating random values from the test dataset. \n",
        "\n",
        "## Further links\n",
        "\n",
        "You can view the project on GitHub for more details.\n",
        "\n",
        "[View the project on GitHub](https://github.com/subhangi03/AgriSage)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
